# Preparing data to answer causal questions {#sec-data-causal}

{{< include 00-setup.qmd >}}

## Introduction to the data {#sec-data}

Throughout this book we will be using data obtained from [Touring Plans](https://touringplans.com).
Touring Plans is a company that helps folks plan their trips to Disney and Universal theme parks.
One of their goals is to accurately predict attraction wait times at these theme parks by leveraging data and statistical modeling.
The `{touringplans}` R package includes several datasets containing information about Disney theme park attractions.
A summary of the attractions included in the package can be found by running the following:

```{r}
library(touringplans)
attractions_metadata
```

Additionally, this package contains a dataset with raw metadata about the parks, with observations recorded daily.
This metadata includes information like the Walt Disney World ticket season on the particular day (was it high season -- think Christmas -- or low season -- think right when school started), what the historic temperatures were in the park on that day, and whether there was a special event, such as "extra magic hours" in the park on that day (did the park open early to guests staying in the Walt Disney World resorts?).

```{r}
parks_metadata_raw
```

Suppose the causal question of interest is:

**Did the decision to provide "Extra Magic Hours" in the morning at Magic Kingdom affect the average wait time for an attraction called the "Seven Dwarfs Mine Train" the same day between 9am and 10am in 2018?**

Let's begin by diagramming this causal question (@fig-seven-diag).

```{r}
#| echo: false
#| fig-cap: "Diagram of the causal question \"Did the decision to provide \"Extra Magic Hours\" in the morning at Magic Kingdom affect the average wait time for an attraction called the \"Seven Dwarfs Mine Train\" the same day between 9am and 10am in 2018?\""
#| label: fig-seven-diag

data <- data.frame(
  labels = c("Extra Magic Hours", "Change average wait time", "Magic Kingdom guests", "before the park opens (2018)", "Magic Kingdom guests", "9am and 10am (2018)"),
  x = c(1, 2, .83, 1.4, 1.88, 2.45),
  y = c(1, 1, 0.77, 0.7, 0.77, 0.7),
  angle = c(0, 0, -52, 0, -52, 0)
)

ggplot(data, aes(x = x, y = y)) +
  geom_text(aes(label = labels, angle = angle, vjust = 0), size = 4) +
  geom_segment(aes(x = 0.5, xend = 2.5, y = 0.95, yend = 0.95)) +
  geom_segment(aes(x = 1.5, xend = 1.5, y = 0.95, yend = 1.1)) +
  geom_segment(aes(x = 0.5, xend = 1, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1, xend = 1.5, y = 0.65, yend = 0.65)) +
  geom_segment(aes(x = 1.55, xend = 2.05, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 2.05, xend = 2.55, y = 0.65, yend = 0.65)) +
  xlim(c(0.5, 2.75)) +
  ylim(c(0.5, 1.2)) +
  theme_void()
```

Historically, guests who stayed in a Walt Disney World resort hotel could access the park during "Extra Magic Hours," during which the park was closed to all other guests.
These extra hours could be in the morning or evening.
The Seven Dwarfs Mine Train is a ride at Walt Disney World's Magic Kingdom.
Magic Kingdom may or may not be selected each day to have these "Extra Magic Hours." We are interested in examining the relationship between whether there were "Extra Magic Hours" in the morning and the average wait time for the Seven Dwarfs Mine Train on the same day between 9 am and 10 am.
Below is a proposed DAG for this question.

```{r}
#| label: fig-dag-magic
#| echo: false
#| message: false
#| warning: false
#| fig.cap: >
#|   Proposed DAG for the relationship between Extra Magic Hours
#|   in the morning at a particular park and the average wait
#|   time between 9 am and 10 am.
#|   The decision to schedule Extra Magic Hours on the day of interest was made
#|   3 months prior.
#|   Here we are saying that we believe 1) Extra Magic Hours impacts average wait time and 2) both Extra Magic Hours and average wait time are determined by the time the park closes, historic high temperatures, and ticket season.

library(tidyverse)
library(ggdag)
library(ggokabeito)

coord_dag <- list(
  x = c(Season = 0, close = 0, weather = -1, x = 1, y = 2),
  y = c(Season = -1, close = 1, weather = 0, x = 0, y = 0)
)

labels <- c(
  x = "Extra Magic Morning",
  y = "Average wait",
  Season = "Ticket Season",
  weather = "Historic high temperature",
  close = "Time park closed"
)

dagify(
  y ~ x + close + Season + weather,
  x ~ weather + close + Season,
  coords = coord_dag,
  labels = labels,
  exposure = "x",
  outcome = "y"
) |>
  tidy_dagitty() |>
  node_status() |>
  ggplot(
    aes(x, y, xend = xend, yend = yend, color = status)
  ) +
  geom_dag_edges_arc(curvature = c(rep(0, 5), .3)) +
  geom_dag_point() +
  geom_dag_label_repel(seed = 1630) +
  scale_color_okabe_ito(na.value = "grey90") +
  theme_dag() +
  theme(
    legend.position = "none",
    axis.text.x = element_text()
  ) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(
    limits = c(-1.25, 2.25),
    breaks = c(-1, 0, 1, 2),
    labels = c(
      "\n(one year ago)",
      "\n(6 months ago)",
      "\n(3 months ago)",
      "9am - 10am\n(Today)"
    )
  )
```

Since we are not in charge of Walt Disney World's operations, we cannot randomize dates to have (or not) "Extra Magic Hours", therefore, we need to rely on previously collected observational data and do our best to emulate the *target trial* that we would have created, should it have been possible.
Here, our observations are *days*.
Looking at the diagram above, we can map each element of the causal question to elements of our target trial protocol:

-   **Eligibility criteria**: Days must be from 2018
-   **Exposure definition**: Magic Kingdom's decision to schedule "Extra Magic Hours" in the morning (a decision made 3 months prior to the morning of interest)
-   **Assignment procedures**: Observed -- if the historic data suggests there were "Extra Magic Hours" in the morning on a particular day, that day is classified as "exposed" otherwise it is "unexposed"
-   **Follow-up period**: From park open to 10am.
-   **Outcome definition**: The average posted wait time between 9am and 10am
-   **Causal contrast of interest**: Average treatment effect (we will discuss this in @sec-estimands)
-   **Analysis plan**: We use inverse probability waiting after fitting a propensity score model to estimate the average treatment effect of the exposure on the outcome of interest. We will adjust for variables as determined by our DAG (@fig-dag-magic)

## Data wrangling and recipes

Most of our data manipulation tools come from the `{dplyr}` package (@tbl-dplyr).
We will also use `{lubridate}` to help us manipulate dates.

| Target trial protocol element | {dplyr} functions                           |
|-------------------------------|---------------------------------------------|
| Eligibility criteria          | `filter()`                                  |
| Exposure definition           | `mutate()`                                  |
| Assignment procedures         | `mutate()`                                  |
| Follow-up period              | `mutate()` `pivot_longer()` `pivot_wider()` |
| Outcome definition            | `mutate()`                                  |
| Analysis plan                 | `select()` `mutate()`                       |

: Mapping target trial protocol elements to commonly used `{dplyr}` functions {#tbl-dplyr}

To answer this question, we are going to need to manipulate both the `seven_dwarfs_train` dataset as well as the `parks_metadata_raw` dataset.
Let's start with the `seven_dwarfs_train` data set.
The Seven Dwarfs Mine Train ride is an attraction at Walt Disney World's Magic Kingdom.
The `seven_dwarfs_train` dataset in the {touringplans} package contains information about the date a particular wait time was recorded (`park_date`), the time of the wait time (`wait_datetime`), the actual wait time (`wait_minutes_actual`), and the posted wait time (`wait_minutes_posted`).
Let's take a look at this dataset.
The {skimr} package is great for getting a quick glimpse at a new dataset.

```{r}
library(skimr)
skim(seven_dwarfs_train)
```

Examining the output above, we learn that this dataset contains four columns and 321,631 rows.
We also learn that the dates span from 2015 to 2021.
We can also examine the distribution of each of the variables to detect any potential anomalies.
Notice anything strange?
Look at the `p0` (that is the minimum value) for `wait_minutes_actual`.
It is `-92918`!
We are not using this variable for this analysis, but we will for future analyses, so this is good to keep in mind.

We need this dataset to calculate our *outcome*.
Recall from above that our outcome is defined as the average posted wait time between 9am and 10am.
Additionally, recall our eligibility criteria states that we need to restrict our analysis to days in 2018.

```{r}
#| message: false
#| warning: false
library(dplyr)
library(lubridate)
seven_dwarfs_train_2018 <- seven_dwarfs_train |>
  filter(year(park_date) == 2018) |> # eligibility criteria
  mutate(hour = hour(wait_datetime)) |> # get hour from wait
  group_by(park_date, hour) |> # group by day and hour
  summarise(
    wait_minutes_posted_avg = mean(wait_minutes_posted, na.rm = TRUE),
    .groups = "drop"
  ) |> # get average wait time
  mutate(
    wait_minutes_posted_avg =
      case_when(
        is.nan(wait_minutes_posted_avg) ~ NA,
        .default = wait_minutes_posted_avg
      )
  ) |> # if it is NAN make it NA
  filter(hour == 9) # only keep the average wait time between 9 and 10
```

```{r}
seven_dwarfs_train_2018
```

Now that we have our outcome settled, we need to get our exposure variable, as well as any other park-specific variables about the day in question that may be used as variables that we adjust for.
Examining @fig-dag-magic, we see that we need data for three proposed confounders: the ticket season, the time the park closed, and the historic high temperature.
These are in the `parks_metadata_raw` dataset.
This data will require extra cleaning, since the names are in the original format.

::: callout-tip
We like to have our variable names follow a clean convention -- one way to do this is to follow Emily Riederer's "Column Names as Contracts" format [@Riederer_2020].
The basic idea is to predefine a set of words, phrases, or stubs with clear meanings to index information, and use these consistently when naming variables.
For example, in these data, variables that are specific to a particular wait time are prepended with the term `wait` (e.g. `wait_datetime` and `wait_minutes_actual`), variables that are specific to the park on a particular day, acquired from parks metadata, are prepended with the term `park` (e.g. `park_date` or `park_temperature_high`).
:::

Let's first decide what variables we will need.
In practice, this decision may involve an iterative process.
For example, after drawing our DAG or conducting diagnostics, we may determine that we need more variables than what we originally cleaned.
Let's start by skimming this dataframe.

```{r}
skim(parks_metadata_raw)
```

This dataset contains many more variables than the one we worked with previously.
For this analysis, we are going to select and rename `date` (the observation date), `wdw_ticket_season` (the ticket season for the observation), `wdwmaxtemp` (the maximum temperature), `mkclose` (the time Magic Kingdom closed), `mkemhmorn` (whether Magic Kingdom had an "Extra Magic Hour" in the morning).

```{r}
parks_metadata_clean <- parks_metadata_raw |>
  ## based on eligibility criteria, limit to 2018
  filter(year(date) == 2018) |>
  ## based on our analysis plan, we will select (and rename) the following variables
  select(
    park_date = date,
    park_ticket_season = wdw_ticket_season,
    park_temperature_high = wdwmaxtemp,
    park_close = mkclose,
    park_extra_magic_morning = mkemhmorn
  )
```

## Working with multiple data sources

Frequently we find ourselves merging data from multiple sources when attempting to answer causal questions in order to ensure that all of the necessary factors are accounted for.
The way we can combine datasets is via *joins* -- joining two or more datasets based on a set or sets of common variables.
We can think of three main types of *joins*: left, right, and inner.
A *left* join combines data from two datasets based on a common variable and includes all records from the *left* dataset along with matching records from the *right* dataset (in `{dplyr}`, `left_join()`), while a *right* join includes all records from the *right* dataset and their corresponding matches from the *left* dataset (in `{dplyr}` `right_join()`); an inner join, on the other hand, includes only the records with matching values in *both* datasets, excluding non-matching records (in `{dplyr}` `inner_join()`.
The variable(s) used as the key for matching across datasets is specified in the `by =` argument.
For this analysis, we will use a left join to pull in the cleaned parks metadata.

```{r}
seven_dwarfs_train_2018 <- seven_dwarfs_train_2018 |>
  left_join(parks_metadata_clean, by = "park_date")
```

## Recognizing missing data

It is important to recognize whether we have any missing data in our variables.
The `{visdat}` package is great for getting a quick sense of whether we have any missing data.

```{r}
library(visdat)
vis_miss(seven_dwarfs_train_2018)
```

It looks like we only have a few observations (2%) missing our outcome of interest.
This is not too bad.
For this first analysis we will ignore the missing values.
We can explicitly drop them using the `drop_na()` function from `{tidyr}`.

```{r}
library(tidyr)
seven_dwarfs_train_2018 <- seven_dwarfs_train_2018 |>
  drop_na()
```

## Exploring and visualizing data and assumptions

The *positivity* assumption requires that within each level and combination of the study variables used to achieve exchangeability, there are exposed and unexposed subjects (@sec-assump).
We can explore this by visualizing the distribution of each of our proposed confounders stratified by the exposure.

### Single variable checks for positivity violations

@fig-close shows the distribution of Magic Kingdom park closing time by whether the date had extra magic hours in the morning.
There is not clear evidence of a lack of positivity here as both exposure levels span the majority of the covariate space.

```{r}
#| label: fig-close
#| fig-cap: "Distribution of Magic Kingdom park closing time by whether the date had extra magic hours in the morning"
ggplot(
  seven_dwarfs_train_2018,
  aes(
    x = factor(park_close),
    group = factor(park_extra_magic_morning),
    fill = factor(park_extra_magic_morning)
  )
) +
  geom_bar(position = position_dodge2(width = 0.9, preserve = "single")) +
  labs(
    fill = "Extra Magic Morning",
    x = "Time of Park Close"
  )
```

To examine the distribution of historic temperature high at Magic Kingdom by whether the date had extra magic hours in the morning we can use a mirrored histogram.
We'll use the {halfmoon} package's `geom_mirror_histogram()` to create one.
Examining @fig-temp, it does look like there are very few days in the exposed group with maximum temperatures less than 60 degrees, while not necessarily a positivity violation it is worth keeping an eye on, particularly because the dataset is not very large, so this could make it difficult to estimate an average exposure effect across this whole space.
If we found this to be particularly difficult, we could posit changing our causal question to instead restrict the analysis to warmer days.
This of course would also restrict which days we could draw conclusions about for the future.

```{r}
#| label: fig-temp
#| fig-cap: "Distribution of historic temperature high at Magic Kingdom by whether the date had extra magic hours in the morning"
library(halfmoon)
ggplot(
  seven_dwarfs_train_2018, 
  aes(
    x = park_temperature_high,
    group = factor(park_extra_magic_morning),
    fill = factor(park_extra_magic_morning)
  )
) +
  geom_mirror_histogram(bins = 20) +
  labs(
    fill = "Extra Magic Morning",
    x = "Historic maximum temperature (degrees F)"
  )
```

Finally, let's look at the distribution of ticket season by whether there were extra magic hours in the morning.
Examining @fig-ticket, we do not see any positivity violations.

```{r}
#| label: fig-ticket
#| fig-cap: "Distribution of historic temperature high at Magic Kingdom by whether the date had extra magic hours in the morning"
ggplot(
  seven_dwarfs_train_2018,
  aes(
    x = park_ticket_season,
    group = factor(park_extra_magic_morning),
    fill = factor(park_extra_magic_morning)
  )
) +
  geom_bar(position = "dodge") +
  labs(
    fill = "Extra Magic Morning",
    x = "Magic Kingdom Ticket Season"
  )
```

### Multiple variable checks for positivity violations

We have confirmed that for each of the three confounders, we do not see strong evidence of positivity violations.
Because we have so few variables here, we can examine this a bit more closely.
Let's start by discretizing the `park_temperature_high` variable a bit (we will cut it into tertiles).

```{r}
#| label: fig-positivity
#| fig-cap: "Check for positivity violations across three confounders: historic high temperature, park close time, and ticket season."
#| fig-width: 9
seven_dwarfs_train_2018 |>
  ## cut park_temperature_high into tertiles
  mutate(park_temperature_high_bin = ntile(park_temperature_high, 3)) |>
  ## bin park close time
  mutate(park_close_bin = case_when(
    hour(park_close) %in% 13:18 ~ "(1) early",
    hour(park_close) %in% 19:23 ~ "(2) standard",
    hour(park_close) %in% c(0:11, 24) ~ "(3) late",
    .default = NA_character_
  )) |>
  group_by(park_close_bin, park_temperature_high_bin, park_ticket_season) |>
  ## calculate the proportion exposed in each bin
  summarise(prop_exposed = mean(park_extra_magic_morning), .groups = "drop") |>
  ggplot(aes(x = park_close_bin, y = park_temperature_high_bin, fill = prop_exposed)) +
  geom_tile() +
  scale_fill_gradient2(midpoint = 0.5) +
  facet_wrap(~park_ticket_season) +
  labs(
    y = "Historic Maximum Temperature (F)",
    x = "Magic Kingdom Park Close Time",
    fill = "Proportion of Days Exposed"
  )
```

Interesting!
@fig-positivity shows an interesting potential violation.
It looks like 100% of days with lower temperatures (historic highs between 51 and 65 degrees) that are in the peak ticket season have extra magic hours in the morning.
This actually makes sense if we think a bit about this data set.
The only days with cold temperatures in Florida that would also be considered a "peak" time to visit Walt Disney World would be over Christmas / New Years.
During this time there historically were always extra magic hours.

We are going to proceed with the analysis, but we will keep these observations in mind.

## Presenting descriptive statistics

Let's examine a table of the variables of interest in this data frame.
To do so, we are going to use the `tbl_summary()` function from the `{gtsummary}` package.
(We'll also use the `{labelled}` package to clean up the variable names for the table.)

```{r}
#| label: tbl-unweighted-gtsummary
#| tbl-cap: A descriptive table of Extra Magic Morning in the touringplans dataset. This table shows the distributions of these variables in the observed population.
library(gtsummary)
library(labelled)
seven_dwarfs_train_2018 <- seven_dwarfs_train_2018 |>
  set_variable_labels(
    park_ticket_season = "Ticket Season",
    park_close = "Close Time",
    park_temperature_high = "Historic High Temperature"
  )

tbl_summary(
  seven_dwarfs_train_2018,
  by = park_extra_magic_morning,
  include = c(park_ticket_season, park_close, park_temperature_high)
) |>
  # add an overall column to the table
  add_overall(last = TRUE)
```
