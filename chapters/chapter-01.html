<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.298">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Causal Inference in R - 1&nbsp; What is a causal question?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/chapter-02.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script defer="" data-domain="r-causal.org" src="https://plausible.io/js/script.js"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/chapter-01.html">Asking Causal Questions</a></li><li class="breadcrumb-item"><a href="../chapters/chapter-01.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What is a causal question?</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Causal Inference in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/malcolmbarrett/causal-inference-in-r" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Asking Causal Questions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-01.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What is a causal question?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The whole game: mosquito nets and malaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Estimating counterfactuals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Target Trials and Standard Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Expressing causal questions as DAGs</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">The Design Phase</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Preparing data to answer causal questions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Building propensity score models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Using the propensity score</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluating your propensity score model</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Estimating Causal Effects</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Causal estimands</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Fitting the outcome model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Continuous exposures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Categorical exposures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Missingness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">G-Computation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Causal survival analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-17.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Causal mediation analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-18.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Sensitivity analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-19.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Other approaches to causal inference</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/references-99.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#schr%C3%B6dingers-causality" id="toc-schrödingers-causality" class="nav-link active" data-scroll-target="#schr%C3%B6dingers-causality"><span class="header-section-number">1.1</span> Schrödinger’s Causality</a></li>
  <li>
<a href="#description-prediction-and-explanation" id="toc-description-prediction-and-explanation" class="nav-link" data-scroll-target="#description-prediction-and-explanation"><span class="header-section-number">1.2</span> Description, prediction, and explanation</a>
  <ul class="collapse">
<li><a href="#description" id="toc-description" class="nav-link" data-scroll-target="#description"><span class="header-section-number">1.2.1</span> Description</a></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction"><span class="header-section-number">1.2.2</span> Prediction</a></li>
  <li><a href="#causal-inference" id="toc-causal-inference" class="nav-link" data-scroll-target="#causal-inference"><span class="header-section-number">1.2.3</span> Causal Inference</a></li>
  <li><a href="#sec-pred-or-explain" id="toc-sec-pred-or-explain" class="nav-link" data-scroll-target="#sec-pred-or-explain"><span class="header-section-number">1.2.4</span> Why isn’t the right causal model just the best prediction model?</a></li>
  </ul>
</li>
  <li><a href="#sec-diag" id="toc-sec-diag" class="nav-link" data-scroll-target="#sec-diag"><span class="header-section-number">1.3</span> Diagraming a causal claim</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/malcolmbarrett/causal-inference-in-r/edit/main/chapters/chapter-01.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/malcolmbarrett/causal-inference-in-r/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/chapter-01.html">Asking Causal Questions</a></li><li class="breadcrumb-item"><a href="../chapters/chapter-01.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What is a causal question?</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-causal-question" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What is a causal question?</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><section id="schrödingers-causality" class="level2" data-number="1.1"><h2 data-number="1.1" class="anchored" data-anchor-id="schrödingers-causality">
<span class="header-section-number">1.1</span> Schrödinger’s Causality</h2>
<p>The heart of causal analysis is the causal question; it dictates what data we analyze, how we analyze it, and to which populations our inferences apply. This book, being applied in nature, deals primarily with the analysis stage of causal inference. Relative to the complexity of specifying a good causal question, the analysis stage is considerably more straightforward. In the first six chapters of this book, we’ll discuss what a causal question is, how to improve our questions, and consider some examples.</p>
<p>Causal questions are part of a broader set of questions we can ask with statistical techniques related to the primary tasks of data science: description, prediction, and causal inference <span class="citation" data-cites="hernan2019">(<a href="#ref-hernan2019" role="doc-biblioref">Hernán, Hsu, and Healy 2019</a>)</span>. Unfortunately, these tasks are often muddled by the techniques we use (regression, for instance, is helpful for all three tasks) and how we talk about them. When researchers are interested in causal inference from non-randomized data, we often use euphemistic language like “association” instead of declaring our intent to estimate a causal effect <span class="citation" data-cites="Hernan2018">(<a href="#ref-Hernan2018" role="doc-biblioref">Hernán 2018</a>)</span>.</p>
<p>In a recent study of the language of analyses in epidemiologic research, for instance, the most common root word describing the estimated effect was “associate,” but many researchers also felt that “associate” implied at least <em>some</em> causal effect (<a href="#fig-word-ranking" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-word-ranking</span></a>) <span class="citation" data-cites="haber_causal_language">(<a href="#ref-haber_causal_language" role="doc-biblioref">Haber et al. 2022</a>)</span>. Only around 1% of the studies analyzed used the root word “cause” at all. Yet, a third of studies had action recommendations, and researchers rated 80% of these recommendations as having at least some causal implication. Often, these studies have stronger action recommendations (alluding to causal effects) than those implied by the description of the effect (root words like “associate” and “compare”). Despite how many studies implied that the goal was causal inference, only about 4% used formal causal models like those discussed in this book. However, most discussed how such a cause might be justified by previous research or theory.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-word-ranking" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="chapter-01_files/figure-html/fig-word-ranking-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;1.1: Rankings of causal strength of root words used by researchers. Root words with more Strong rankings have stronger causal implications than those with many None or Weak rankings. Data from Haber et al.</figcaption></figure>
</div>
</div>
</div>
<p>Instead of clear questions with obvious assumptions and goals, we end up with “Schrödinger’s causal inference”:</p>
<blockquote class="blockquote">
<p>Our results suggest that “Schrödinger’s causal inference,” — where studies avoid stating (or even explicitly deny) an interest in estimating causal effects yet are otherwise embedded with causal intent, inference, implications, and recommendations — is common.</p>
<p>— <span class="citation" data-cites="haber_causal_language">Haber et al. (<a href="#ref-haber_causal_language" role="doc-biblioref">2022</a>)</span></p>
</blockquote>
</section><section id="description-prediction-and-explanation" class="level2" data-number="1.2"><h2 data-number="1.2" class="anchored" data-anchor-id="description-prediction-and-explanation">
<span class="header-section-number">1.2</span> Description, prediction, and explanation</h2>
<p>An excellent first step to address this problem is recognizing that questions about description, prediction, and explanation are fundamentally different. Data science in industry isn’t quite as burdened by Schrödinger’s causal inference as the academic sciences, but being explicit in the differences in analytic intent is still helpful. For instance, when a stakeholder asks for “drivers” of a particular event, what are they asking? For a model to predict the event? For a deeper understanding of what causes the event? It’s a vague request, but it smacks of causal interest to us. When we’re clear about our goals, we can use all three approaches more effectively (and, as we’ll see, both descriptive analysis and prediction models are still helpful when the goal is to make causal inferences). Moreover, all three approaches are useful decision-making tools.</p>
<section id="description" class="level3" data-number="1.2.1"><h3 data-number="1.2.1" class="anchored" data-anchor-id="description">
<span class="header-section-number">1.2.1</span> Description</h3>
<p>Descriptive analysis aims to describe the distribution of variables, often stratified by key variables of interest. A closely related idea is exploratory data analysis (EDA), although descriptive studies often have more explicit goals than those in EDA.</p>
<p>Descriptive analyses are usually based on statistical summaries such as measures of centrality (means, medians) and spread (minimums, maximums, quartiles), but they also occasionally use techniques like regression modeling. The goal of applying more advanced techniques like regression is different in descriptive analyses than in either predictive or causal studies. “Adjusting” for a variable in descriptive analyses means that we are removing its associational effect (and thus changing our question), <em>not</em> that we are controlling for confounding.</p>
<p>In epidemiology, a valuable concept for descriptive analyses is “person, place, and time” – who has what disease, where, and when. This concept is also a good template for descriptive analyses in other fields. Usually, we want to be clear about what population we’re trying to describe, so we need to be as specific as possible. For human health, describing the people involved, the location, and the period are all critical. In other words, focus on the first principles of generating understanding of your data and describe your data accordingly.</p>
<section id="examples" class="level4" data-number="1.2.1.1"><h4 data-number="1.2.1.1" class="anchored" data-anchor-id="examples">
<span class="header-section-number">1.2.1.1</span> Examples</h4>
<p>Counting things is one of the best things we can do with data. EDA benefits both predictive and causal analyses, but descriptive analyses are valuable independent of the other analysis tasks. Ask any data scientist who thought they’d be developing complex machine learning models and found themselves spending most of their time on dashboards. Understanding the distributions of the data, particularly for key analysis goals (say, KPIs in industry or disease incidence in epidemiology), is critical for many types of decision-making.</p>
<p>One of the best recent examples of descriptive analyses arose from the COVID-19 pandemic <span class="citation" data-cites="Fox2022">(<a href="#ref-Fox2022" role="doc-biblioref">Fox et al. 2022</a>)</span>. In 2020, particularly in the early months of the pandemic, descriptive analyses were vital to understanding risk and allocating resources. Since the coronavirus is similar to other respiratory diseases, we had many public health tools to reduce risk (e.g., distancing and, later, face masks). Descriptive statistics of cases by region were vital for deciding local policies and the strength of those policies.</p>
<p>A great example of a more complex descriptive analysis during the pandemic was an <a href="https://www.ft.com/content/a2901ce8-5eb7-4633-b89c-cbdf5b386938">ongoing report by the Financial Times of expected deaths vs.&nbsp;observed deaths</a> in various countries and regions<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. While the calculation of expected deaths is slightly more sophisticated than most descriptive statistics, it provided a tremendous amount of information about current deaths without needing to untangle causal effects (e.g., were they due to COVID-19 directly? Inaccessible healthcare? Cardiovascular events post-COVID?). In this (simplified) recreation of their plot from July 2020, you can see the staggering effect of the pandemic’s early months.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ft-chart" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="chapter-01_files/figure-html/fig-ft-chart-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption class="figure-caption">Figure&nbsp;1.2: 2020 excess deaths vs.&nbsp;historical expected deaths from any cause. Data from the Financial Times.</figcaption></figure>
</div>
</div>
</div>
<p>Here are some other great examples of descriptive analyses.</p>
<ul>
<li>Deforestation around the world. Our World in Data <span class="citation" data-cites="owidforestsanddeforestation">(<a href="#ref-owidforestsanddeforestation" role="doc-biblioref">Ritchie and Roser 2021</a>)</span> is a data journalism organization that produces thoughtful, usually descriptive reports on various topics. In this report, they present data visualizations of both absolute change in forest coverage (forest transitions) and relative change (deforestation or reforestation), using basic statistics and forestry theory to present helpful information about the state of forests over time.</li>
<li>The prevalence of chlamydial and gonococcal infections <span class="citation" data-cites="Miller2004">(<a href="#ref-Miller2004" role="doc-biblioref">Miller 2004</a>)</span>. Measuring the prevalence of disease (how many people currently have a disease, usually expressed as a rate per number of people) is helpful for basic public health (resources, prevention, education) and scientific understanding. In this study, the authors conducted a complex survey meant to be representative of all high schools in the United States (the target population); they used survey weights to address a variety of factors related to their question, then calculated prevalence rates and other statistics. As we’ll see, weights are helpful in causal inference for the same reason: targeting a particular population. That said, not all weighting techniques are causal in nature, and they were not here.</li>
<li>Estimating race and ethnicity-specific hysterectomy inequalities <span class="citation" data-cites="Gartner2020">(<a href="#ref-Gartner2020" role="doc-biblioref">Gartner et al. 2020</a>)</span>. Descriptive techniques also help us understand disparities in areas like economics and epidemiology. In this study, the authors asked: Does the risk of hysterectomy differ by racial or ethnic background? Although the analysis is stratified by a key variable, it’s still descriptive. Another interesting aspect of this paper is the authors’ work ensuring the research answered questions about the right target population. Their analysis combined several data sources to better estimate the true population prevalence (instead of the prevalence among those in hospitals, as commonly presented). They also adjusted for the prevalence of hysterectomy, e.g., they calculated the incidence (new case) rate only among those who could actually have a hysterectomy (e.g., they hadn’t had one yet).</li>
</ul></section><section id="validity" class="level4" data-number="1.2.1.2"><h4 data-number="1.2.1.2" class="anchored" data-anchor-id="validity">
<span class="header-section-number">1.2.1.2</span> Validity</h4>
<p>There are two critical validity issues in descriptive analyses: measurement and sampling errors.</p>
<p>Measurement error is when we have mismeasured one or more variables in some capacity. For descriptive analyses, mismeasuring things means we may not get the answer to our question. However, the degree to which that is the case depends on both the severity of the measurement error and the question itself.</p>
<p>Sampling error is a more nuanced topic in descriptive analyses. It’s related to the population we’re analyzing (who should the analysis produce descriptions about) and uncertainty (how certain are we that the descriptions of those we have data for represent the population we’re trying to describe.).</p>
<p>The population from which our data come and the population we’re trying to describe must be the same for us to provide valid descriptions. Consider a dataset generated by an online survey. Who are the people who are answering these questions, and how do they relate to the people we want to describe? For many analyses, the people who take the time to answer surveys are different than the people we want to describe, e.g., the group of people who fill out surveys may have a different distribution of variables than those who don’t. Results from data like these are not technically biased because, outside of sample size-related uncertainty and measurement error, the descriptions are accurate—they’re just not for the right group of people! In other words, <em>we’ve gotten an answer to the wrong question</em>.</p>
<p>Notably, sometimes our data represent the entire population (or close enough) that sampling error is irrelevant. Consider a company with certain data on every customer using their service. For many analyses, this represents the entire population (current customers) about whom we want information. Similarly, in countries with population-wide health registries, the data available for specific practical purposes is close enough to the entire population that no sampling is needed (although researchers might use sampling for simpler computations). In these cases, there’s not really such a thing as uncertainty. Assuming everything is well-measured, the summary statistics we generate <em>are inherently unbiased and precise</em> because we have information from everyone in the population. Of course, in practice, we usually have some mixture of measurement error, missing data, and so on, even in the best of circumstances.</p>
<p>One crucial detail of descriptive analysis is that confounding bias, one of the chief concerns of this book, is undefined. That’s because confounding is a causal concern. Descriptive analyses cannot be confounded because they are a statistical description of relationships <em>as-is</em>, not the mechanisms behind those relationships.</p>
</section><section id="relationship-to-causal-inference" class="level4" data-number="1.2.1.3"><h4 data-number="1.2.1.3" class="anchored" data-anchor-id="relationship-to-causal-inference">
<span class="header-section-number">1.2.1.3</span> Relationship to causal inference</h4>
<p>Humans see patterns very well. Pattern-finding is a helpful feature of our brains, but it can also lead down a slippery slope of inference when we’re not working with data or methods that can allow us to do that validly. The biggest thing to be cautious of when your goal is to describe is making the leap from description to causation (implicitly or explicitly).</p>
<p>But, of course, descriptive analysis is helpful when we <em>are</em> estimating causal effects. It helps us understand the population we’re working with, the distribution of the outcomes, exposures (variables we think might be causal), and confounders (variables we need to account for to get unbiased causal effects for the exposure). It also helps us be sure that the data structure we’re using matches the question we’re trying to answer, as we’ll see in <a href="#sec-data-causal" class="quarto-xref">Chapter&nbsp;<span class="quarto-unresolved-ref ref-noprefix">sec-data-causal</span></a>. You should always do descriptive analyses of your data when conducting causal research.</p>
<p>Finally, as we’ll see in <a href="#sec-trials-std" class="quarto-xref">Chapter&nbsp;<span class="quarto-unresolved-ref ref-noprefix">sec-trials-std</span></a>, there are certain circumstances where we can make causal inferences with basic statistics. Be cautious about the distinction between the causal question and the descriptive component here, too: just because we’re using the same calculation (e.g., a difference in means) doesn’t mean that all descriptions you can generate are causal. Whether a descriptive analysis overlaps with a causal analysis is a function of the data and the question.</p>
</section></section><section id="prediction" class="level3" data-number="1.2.2"><h3 data-number="1.2.2" class="anchored" data-anchor-id="prediction">
<span class="header-section-number">1.2.2</span> Prediction</h3>
<p>The goal of prediction is to use data to make accurate predictions about variables, usually on new data. What this means depends on the question, domain, and so on. Prediction models are used in about every setting imaginable, from peer-reviewed clinical models to bespoke machine learning models embedded in consumer devices. Even large language models like the ones ChatGPT is based on are prediction models: they predict what a response to a prompt would look like.</p>
<p>Predictive modeling generally uses a different workflow than the workflow for causal modeling we’ll present in this book. Since the goal of prediction is usually related to making predictions on new data, the workflow of this type of modeling focuses on maximizing predictive accuracy while retaining generalization to new data, sometimes called the bias-variance trade-off. In practice, this means splitting your data into training sets (the part of the data you build your model on) and test sets (the part you assess your model with, a proxy for how it would perform on new data). Usually, data scientists use cross-validation or other sampling techniques to reduce further the risk of overfitting your model to the training set.</p>
<p>There are many excellent texts on predictive modeling, and so we refer you to those for a deeper exploration of the goals and methods of these techniques <span class="citation" data-cites="kuhn2013a harrell2001 Kuhn_Silge_2022 James_Witten_Hastie_Tibshirani_2022">(<a href="#ref-kuhn2013a" role="doc-biblioref">Kuhn and Johnson 2013</a>; <a href="#ref-harrell2001" role="doc-biblioref">Harrell 2001</a>; <a href="#ref-Kuhn_Silge_2022" role="doc-biblioref">Kuhn and Silge 2022</a>; <a href="#ref-James_Witten_Hastie_Tibshirani_2022" role="doc-biblioref">James et al. 2022</a>)</span>.</p>
<section id="examples-1" class="level4" data-number="1.2.2.1"><h4 data-number="1.2.2.1" class="anchored" data-anchor-id="examples-1">
<span class="header-section-number">1.2.2.1</span> Examples</h4>
<p>Prediction is the most popular topic in data science, largely thanks to machine learning applications in industry. Prediction, of course, has a long history in statistics, and many models popular today have been used for decades in and outside academia.</p>
<p>Let’s look at an example of prediction about COVID-19 <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. In 2021, researchers published the ISARIC 4C Deterioration model, a clinical prognostic model for predicting severe adverse outcomes for acute COVID-19 <span class="citation" data-cites="Gupta2021">(<a href="#ref-Gupta2021" role="doc-biblioref">Gupta et al. 2021</a>)</span>. The authors included a descriptive analysis to understand the population from which this model was developed, particularly the distribution of the outcome and candidate predictors. One helpful aspect of this model is that it uses items commonly measured on day one of COVID-related hospitalization. The authors built this model using cross-validation by region of the UK and then tested the model on data from a hold-out region. The final model included eleven items and a description of their model attributes, relation with the outcome, and so on. Notably, the authors used clinical domain knowledge to select candidate variables but did not fall into the temptation of interpreting the model coefficients as causal. Without question, some of the predictive value of this model stems from the causal structure of the variables as they relate to the outcome, but the researchers had a different goal entirely for this model and stuck to it.</p>
<p>Here are other good examples from the predictive space:</p>
<ul>
<li><p>Some of the most exciting work in predictive modeling is in industry. Netflix regularly shares details on their modeling success and novel strategies in their <a href="https://research.netflix.com/">research blog</a>. They also recently published a paper reviewing their use of deep learning models for recommender systems (in this case, recommending shows and movies to users) <span class="citation" data-cites="steck2021">(<a href="#ref-steck2021" role="doc-biblioref">Steck et al. 2021</a>)</span>. The authors explain their experimentation with models, the details of those models, and many of the challenges they faced, resulting in a practical guide on using such models.</p></li>
<li><p>In early 2020, researchers experienced with predictive and prognostic modeling in health research published a review of models for diagnosis and prognosis of COVID-19 <span class="citation" data-cites="Wynants2020">(<a href="#ref-Wynants2020" role="doc-biblioref">Wynants et al. 2020</a>)</span>. This review is interesting not just for its breadth but also the astounding number of models that were rated as poor quality: “[232] models were rated at high or unclear risk of bias, mostly because of non-representative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, high risk of model overfitting, and unclear reporting.” This research is also a <a href="https://www.covprecise.org/">living review</a>.</p></li>
</ul></section><section id="validity-1" class="level4" data-number="1.2.2.2"><h4 data-number="1.2.2.2" class="anchored" data-anchor-id="validity-1">
<span class="header-section-number">1.2.2.2</span> Validity</h4>
<p>The key measure of validity in prediction modeling is predictive accuracy, which can be measured in several ways, such as root mean squared error (RMSE), mean absolute error (MAE), area under the curve (AUC), and many more. A convenient detail about predictive modeling is that we can often assess if we’re right, which is not true of descriptive statistics for which we only have a subset of data or causal inference for which we don’t know the true causal structure. We aren’t always able to assess against the truth, but it’s almost always required for fitting the initial predictive model <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>Measurement error is also a concern for predictive modeling because we usually need accurate data for accurate predictions. Interestingly, measurement error and missingness can be informative in predictive settings. In a causal setting, this might induce bias, but predictive models can consume that information with no issue. For instance, in the famous Netflix Prize, winning models leveraged information about whether or not a customer rated a movie at all to improve recommendation systems.</p>
<p>Like descriptive analysis, confounding is undefined for predictive modeling. A coefficient in a prediction model cannot be confounded; we only care about whether or not the variable provides predictive information, not if that information is because of a causal relationship or something else.</p>
</section><section id="relationship-to-causal-inference-1" class="level4" data-number="1.2.2.3"><h4 data-number="1.2.2.3" class="anchored" data-anchor-id="relationship-to-causal-inference-1">
<span class="header-section-number">1.2.2.3</span> Relationship to causal inference</h4>
<p>The single biggest risk in prediction is to assume that a given coefficient in a model has a causal interpretation. There is a good chance that this isn’t so. A model may predict well but may also have completely biased coefficients from a causal point of view. We’ll see more about this in <a href="#sec-pred-or-explain" class="quarto-xref"><span class="quarto-unresolved-ref">sec-pred-or-explain</span></a> and the rest of the book.</p>
<p>Often, people mistakenly use methods for selecting features (variables) for prediction models to select confounders in causal models. Aside from their risk of overfitting, these methods are appropriate for prediction models but not for causal models. Prediction metrics cannot determine the causal structure of your question, and predictive value for the outcome does not make a variable a confounder. In general, background knowledge (not prediction or associational statistics) should help you select variables for causal models <span class="citation" data-cites="robinsImpossible">Robins and Wasserman (<a href="#ref-robinsImpossible" role="doc-biblioref">1999</a>)</span>; we’ll discuss this process in detail in <a href="#sec-dags" class="quarto-xref">Chapter&nbsp;<span class="quarto-unresolved-ref ref-noprefix">sec-dags</span></a> and <a href="#sec-building-models" class="quarto-xref">Chapter&nbsp;<span class="quarto-unresolved-ref ref-noprefix">sec-building-models</span></a>.</p>
<p>Prediction is nevertheless crucial to causal inference. From a philosophical perspective, we’re comparing predictions from different <em>what if</em> scenarios: What would the outcome had one thing happened vs.&nbsp;if another thing happened? We’ll spend much time on this subject, particularly in <a href="#sec-counterfactuals" class="quarto-xref">Chapter&nbsp;<span class="quarto-unresolved-ref ref-noprefix">sec-counterfactuals</span></a>. We’ll also talk a lot about prediction from a practical perspective: just like in prediction and some description, we’ll use modeling techniques to answer causal questions. Techniques like propensity score methods and g-computation use model predictions to answer causal questions, but the workflow for building and interpreting the models themselves are quite different.</p>
</section></section><section id="causal-inference" class="level3" data-number="1.2.3"><h3 data-number="1.2.3" class="anchored" data-anchor-id="causal-inference">
<span class="header-section-number">1.2.3</span> Causal Inference</h3>
<p>The goal of causal inference is to understand the impact that a variable, sometimes called an exposure, has on another variable, sometimes called an outcome. “Exposure” and “outcome” are the terms we’ll use in this book to describe the causal relationship we’re interested in. Importantly, our goal is to answer this question clearly and precisely. In practice, this means using techniques like study design (e.g., a randomized trial) or statistical methods (like propensity scores) to calculate an unbiased effect of the exposure on the outcome.</p>
<p>As with prediction and description, it’s better to start with a clear, precise question to get a clear, precise answer. In statistics and data science, particularly as we swim through the ocean of data of the modern world, we often end up with an answer without a question (e.g., <code>42</code>). This, of course, makes interpretation of the answer difficult. In <a href="#sec-diag" class="quarto-xref"><span class="quarto-unresolved-ref">sec-diag</span></a>, we’ll discuss the structure of causal questions. We’ll discuss philosophical and practical ways to sharpen our questions in <a href="#sec-counterfactuals" class="quarto-xref">Chapter&nbsp;<span class="quarto-unresolved-ref ref-noprefix">sec-counterfactuals</span></a> and <a href="#sec-trials-std" class="quarto-xref">Chapter&nbsp;<span class="quarto-unresolved-ref ref-noprefix">sec-trials-std</span></a>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Causal inference and explanation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some authors use the phrases “causal inference” and “explanation” interchangeably. We’re a little more cautious about that. Causal inference always has a relationship to explanation, but we can accurately estimate the effect of one thing on another without understanding how it happens.</p>
<p>Consider John Snow, the so-called father of epidemiology. In 1854, Snow famously investigated a cholera outbreak in London and identified that specific water sources were to blame for the disease. He was right: contaminated water was a mechanism for cholera transmission. Yet, he didn’t have enough information to explain how: <em>Vibrio cholerae</em>, the bacteria responsible for cholera, wasn’t identified until nearly thirty years later.</p>
</div>
</div>
<section id="examples-2" class="level4" data-number="1.2.3.1"><h4 data-number="1.2.3.1" class="anchored" data-anchor-id="examples-2">
<span class="header-section-number">1.2.3.1</span> Examples</h4>
<p>We’ll see many examples of causal inference in this book, but let’s continue with an example related to COVID-19. As the pandemic continued and tools like vaccines and anti-viral treatments became available, policies like universal masking also began to change. In February 2022, the US state of Massachusetts rescinded a statewide policy that required universal masking in public schools <span class="citation" data-cites="Cowger2022">(<a href="#ref-Cowger2022" role="doc-biblioref">Cowger et al. 2022</a>)</span>. In the greater Boston area, some school districts continued the policy while others discontinued it; those that discontinued it also did so at different times over the following weeks after the policy change. This difference in policy allowed researchers to take advantage of the differences in district policies over this period to study the impact of universal masking on COVID-19 cases. The researchers included a descriptive analysis of the school districts to understand the distribution of factors related to COVID-19 and other determinants of health. To estimate the effect of universal masking on cases, the authors used a technique common in policy-related causal inference called difference-in-differences to estimate this effect. Their design alleviates some problematic assumptions needed for causal inference, but they also wisely controlled for potential confounders despite that advantage. The authors found that districts that continued masking saw a drastically lower caseload than those that didn’t; their analysis concluded that almost 12,000 additional cases occurred due to the policy change, nearly 30% of the cases in the districts during the 15 weeks of the study.</p>
<!-- TODO: add example when code and data are available -->
<p>Here are a few other interesting examples:</p>
<ul>
<li><p>Netflix regularly uses causal inference in their work. In 2022, they published a <a href="https://netflixtechblog.com/a-survey-of-causal-inference-applications-at-netflix-b62d25175e6f">blog post summarizing some causal tasks</a> they have engaged with. One interesting example is localization. Netflix, being worldwide, localizes content through subtitles and dubbing. Randomized experiments were a bad idea because they meant withholding content from users, so researchers at Netflix used several approaches to understand the value of localization while addressing potential confounding. One example is studying the impact of pandemic-related delays in dubbing. Researchers used synthetic controls to simulate the impact on viewership with and without these delays. Presumably, the timing of the pandemic-related delays was unrelated to many factors that would typically be related to dubbing processes, thus reducing some of the potential confounding.</p></li>
<li><p>The Tuskegee Study is one of modern history’s most infamous examples of medical abuse. It is commonly pointed to as a source of distrust in the medical community from Black Americans. Health economics researchers used a variation of difference-in-difference techniques to assess the effect of the Tuskegee Study on distrust and life expectancy in older Black men <span class="citation" data-cites="Alsan2018">(<a href="#ref-Alsan2018" role="doc-biblioref">Alsan and Wanamaker 2017</a>)</span>. The results are important and disturbing: “We find that the disclosure of the study in 1972 is correlated with increases in medical mistrust and mortality and decreases in both outpatient and inpatient physician interactions for older black men. Our estimates imply life expectancy at age 45 for black men fell by up to 1.5 years in response to the disclosure, accounting for approximately 35% of the 1980 life expectancy gap between black and white men and 25% of the gap between black men and women.”</p></li>
</ul></section><section id="validity-2" class="level4" data-number="1.2.3.2"><h4 data-number="1.2.3.2" class="anchored" data-anchor-id="validity-2">
<span class="header-section-number">1.2.3.2</span> Validity</h4>
<p>Making valid causal inferences requires several assumptions that we’ll discuss in <a href="#sec-assump" class="quarto-xref"><span class="quarto-unresolved-ref">sec-assump</span></a>. Unlike prediction, we generally cannot confirm that our causal models are correct. In other words, most assumptions we need to make are unverifiable. We’ll come back to this topic time and time again in the book—from the basics of these assumptions to practical decision-making to probing our models for problems.</p>
</section></section><section id="sec-pred-or-explain" class="level3" data-number="1.2.4"><h3 data-number="1.2.4" class="anchored" data-anchor-id="sec-pred-or-explain">
<span class="header-section-number">1.2.4</span> Why isn’t the right causal model just the best prediction model?</h3>
<p>At this point, you may wonder why the right causal model isn’t just the best prediction model. It makes sense that the two would be related: naturally, things that cause other things would be predictors. It’s causality all the way down, so any predictive information <em>is</em> related, in some capacity, to the causal structure of the thing we’re predicting. Doesn’t it stand to reason that a model that predicts well is causal, too? It’s true that <em>some</em> predictive models can be great causal models and vice versa. Unfortunately, this is not always the case; causal effects needn’t predict particularly well, and good predictors needn’t be causally unbiased <span class="citation" data-cites="shmueli2010a">(<a href="#ref-shmueli2010a" role="doc-biblioref">Shmueli 2010</a>)</span>. There is no way to know using data alone.</p>
<p>Let’s look at the causal perspective first because it’s a bit simpler. Consider a causally unbiased model for an exposure but only includes variables related to the outcome <em>and</em> the exposure. In other words, this model provides us with the correct answer for the exposure of interest but doesn’t include other predictors of the outcome (which can sometimes be a good idea, as discussed in <a href="#sec-data-causal" class="quarto-xref"><span class="quarto-unresolved-ref">sec-data-causal</span></a>). If an outcome has many causes, a model that accurately describes the relationship with the exposure likely won’t predict the outcome very well. Likewise, if a true causal effect of the exposure on the outcome is small, it will bring little predictive value. In other words, the predictive ability of a model, whether high or small, can’t help us distinguish if the model is giving us the correct answer. Of course, low predictive power might also indicate that a causal effect isn’t much use from an applied perspective, although that depends on several statistical factors.</p>
<p>There are two more complex reasons that predictive models won’t always be unbiased causal models. For the first reason, let’s consider an accurate model from a causal perspective: it estimates effects on an outcome, and all of these effects are unbiased. Even in this ideal setting, you might get better predictions using a different model. The reason has to do with the bias-variance trade-off in predictive modeling. When effects are small, data are noisy, predictors are highly correlated, or there’s not much data, using a biased model like penalized regression might make sense. These models intentionally introduce bias in favor of improved variance in out-of-data predictions. Since the goals of prediction and causal inference are different (accurate predictions, usually for out-of-data observations vs.&nbsp;an unbiased effect), the best model for inference is not necessarily the best prediction model.</p>
<!-- TODO: maybe include a simulation to prove it? -->
<p>Secondly, variables that are biased from a causal perspective often bring along with them predictive power. We’ll discuss which variables to include and <em>not</em> include in your models in <a href="#sec-dags" class="quarto-xref"><span class="quarto-unresolved-ref">sec-dags</span></a>, but let’s consider a simple example. One of the famous examples of confounded relationships is ice cream sales and crime in the summer. <a href="https://slate.com/news-and-politics/2013/07/warm-weather-homicide-rates-when-ice-cream-sales-rise-homicides-rise-coincidence.html">Descriptively, ice cream sales and crime are related</a>, but this relationship is confounded by weather, e.g., both ice cream sales and crime increases when it’s warmer. (This is simplistic, of course, as weather itself doesn’t cause crime, but it’s on the causal pathway.)</p>
<p>Consider a thought experiment where you are in a dark room. Your goal is to predict crime, but you don’t know the weather or time of year. You do, however, have information on ice cream sales. A model with ice cream sales on crime would be biased from a causal perspective—ice cream sales do not cause crime, even though the model would show an effect—but would provide some predictive value to your crime model. The reason for both of these conditions is the same: weather and ice cream sales are correlated, and so are weather and crime. Ice cream sales can successfully, if imperfectly, serve as a proxy for weather. That results in a biased effect estimate of the causal impact of ice cream sales on crime but a partially effective prediction of crime. Other variables, too, which are invalid from a causal perspective, either by being biased themselves or by introducing bias into the causal effect estimate, often bring good predictive value. Thus, predictive accuracy is not a good measure of causality.</p>
<p>A closely related idea is the <em>Table Two Fallacy</em>, so-called because, in health research papers, descriptive analyses are often presented in Table 1, and regression models are often presented in Table 2 <span class="citation" data-cites="Westreich2013">(<a href="#ref-Westreich2013" role="doc-biblioref">Westreich and Greenland 2013</a>)</span>. The Table Two Fallacy is when a researcher presents confounders and other non-effect variables, particularly when they interpret those coefficients as if they, too, were causal. The problem is that in some situations, the model to estimate an unbiased effect of one variable may not be the same model to estimate an unbiased effect of another variable. In other words, we can’t interpret the effects of confounders as causal because they might <em>themselves</em> be confounded by another variable unrelated to the original exposure.</p>
<p>Descriptive, predictive, and causal analyses will always contain some aspect of each other. A predictive model gains some of its predictive power from the causal structure of the outcome, and a causal model has some predictive power because it contains information about the outcome. However, the same model in the same data with different goals will have different usefulness depending on those goals. <!-- TODO: uncomment this when section is written -- We'll dive more deeply into this topic in @sec-causal-pred-revisit. --></p>
</section></section><section id="sec-diag" class="level2" data-number="1.3"><h2 data-number="1.3" class="anchored" data-anchor-id="sec-diag">
<span class="header-section-number">1.3</span> Diagraming a causal claim</h2>
<p>Each analysis task, whether descriptive, predictive, or inferential, should start with a clear, precise question. Let’s diagram them to understand better the structure of causal questions (to which we’ll return our focus). Diagramming sentences is a grammatical method used to visually represent the structure of a sentence, occasionally taught in grammar school. In this technique, sentences are deconstructed into their constituent parts, such as subjects, verbs, objects, and modifiers, and then displayed using a series of lines and symbols. The arrangement of these elements on the diagram reflects their syntactic roles and how they interact within the sentence’s overall structure. By breaking down sentences into these visual representations, diagramming can help learners grasp the nuances of sentence construction, identify grammatical errors, and appreciate the intricate connections between words. We can apply a similar idea to <em>causal claims</em>. Here is an example of how one might diagram a causal claim. We’ve pulled out the <em>cause</em>, the <em>effect</em>, the <em>subject</em> (for whom?), and the <em>timing</em> (when?).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-diagram-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="../images/sentence-diagram-1.png" class="img-fluid figure-img" width="2219"></p>
<figcaption class="figure-caption">Figure&nbsp;1.3: Example of diagraming a causal claim.</figcaption></figure>
</div>
</div>
</div>
<p>Let’s start with a basic causal question: <strong>Does smoking cause lung cancer?</strong></p>
<p>The causal claim here could be that <em>smoking causes lung cancer</em>. <a href="#fig-diagram-2" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-diagram-2</span></a> shows a potential diagram of this causal claim.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-diagram-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="../images/sentence-diagram-2.png" class="img-fluid figure-img" width="2219"></p>
<figcaption class="figure-caption">Figure&nbsp;1.4: Diagram of the causal claim “smoking causes lung cancer”.</figcaption></figure>
</div>
</div>
</div>
<p>Let’s get more specific. A study was published in <em>JAMA</em> (the Journal of the American Medical Association) in 2005 titled “Effect of Smoking Reduction on Lung Cancer Risk.” This study concluded: “Among individuals who smoke 15 or more cigarettes per day, smoking reduction by 50% significantly reduces the risk of lung cancer”. <span class="citation" data-cites="godtfredsen2005effect">(<a href="#ref-godtfredsen2005effect" role="doc-biblioref">Godtfredsen, Prescott, and Osler 2005</a>)</span> The study describes the time frame studied as 5-10 years. Let’s diagram this causal claim. Here, we assume that the eligibility criteria and the target population for the estimated causal effect are the same (individuals who smoke 15 or more cigarettes per day); this need not always be the case. In <a href="#sec-estimands" class="quarto-xref"><span class="quarto-unresolved-ref">sec-estimands</span></a>, we will discuss other potential target populations.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-diagram-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="../images/sentence-diagram-3.png" class="img-fluid figure-img" width="2222"></p>
<figcaption class="figure-caption">Figure&nbsp;1.5: Example diagram of a more specific causal claim based on results from <span class="citation" data-cites="godtfredsen2005effect">Godtfredsen, Prescott, and Osler (<a href="#ref-godtfredsen2005effect" role="doc-biblioref">2005</a>)</span>.</figcaption></figure>
</div>
</div>
</div>
<p>Translating this idea into asking good causal questions, we can map the following terms that you will see throughout this book to these diagrams: <em>exposure</em> (the cause), <em>outcome</em> (the effect), <em>eligibility criteria</em> (for whom?), <em>time zero</em> (when did the participants begin to be followed?), <em>target population</em>, (who can we estimate an outcome effect for?) and <em>follow-up period</em> (when?).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-diagram-4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="../images/sentence-diagram-4.png" class="img-fluid figure-img" width="2219"></p>
<figcaption class="figure-caption">Figure&nbsp;1.6: Example diagram mapped to causal analysis terminology</figcaption></figure>
</div>
</div>
</div>
<p>Asking good causal questions means we map the <em>question</em> to the observable <em>evidence</em>. Let’s return to the smoking example. Our initial question was: <em>Does smoking cause lung cancer?</em>; The evidence in the study shows: <em>For people who smoke 15+ cigarettes a day, reducing smoking by 50% reduces the risk of lung cancer over 5-10 years</em>. Does the answer match the question? Not quite. Let’s update our question to match what the study actually showed: <em>For people who smoke 15+ cigarettes a day, does reducing smoking by 50% reduce the lung cancer risk over 5-10 years?</em> Honing this skill — asking answerable causal questions — is essential and one we will discuss throughout this book.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Alsan2018" class="csl-entry" role="listitem">
Alsan, Marcella, and Marianne Wanamaker. 2017. <span>“Tuskegee and the Health of Black Men*.”</span> <em>The Quarterly Journal of Economics</em> 133 (1): 407–55. <a href="https://doi.org/10.1093/qje/qjx029">https://doi.org/10.1093/qje/qjx029</a>.
</div>
<div id="ref-Cowger2022" class="csl-entry" role="listitem">
Cowger, Tori L., Eleanor J. Murray, Jaylen Clarke, Mary T. Bassett, Bisola O. Ojikutu, Sarimer M. Sánchez, Natalia Linos, and Kathryn T. Hall. 2022. <span>“Lifting Universal Masking in Schools <span></span> Covid-19 Incidence Among Students and Staff.”</span> <em>New England Journal of Medicine</em> 387 (21): 1935–46. <a href="https://doi.org/10.1056/nejmoa2211029">https://doi.org/10.1056/nejmoa2211029</a>.
</div>
<div id="ref-Fox2022" class="csl-entry" role="listitem">
Fox, Matthew P, Eleanor J Murray, Catherine R Lesko, and Shawnita Sealy-Jefferson. 2022. <span>“On the Need to Revitalize Descriptive Epidemiology.”</span> <em>American Journal of Epidemiology</em> 191 (7): 1174–79. <a href="https://doi.org/10.1093/aje/kwac056">https://doi.org/10.1093/aje/kwac056</a>.
</div>
<div id="ref-Gartner2020" class="csl-entry" role="listitem">
Gartner, Danielle R., Paul L. Delamater, Robert A. Hummer, Jennifer L. Lund, Brian W. Pence, and Whitney R. Robinson. 2020. <span>“Integrating Surveillance Data to Estimate Race/Ethnicity-Specific Hysterectomy Inequalities Among Reproductive-Aged Women.”</span> <em>Epidemiology</em> 31 (3): 385–92. <a href="https://doi.org/10.1097/ede.0000000000001171">https://doi.org/10.1097/ede.0000000000001171</a>.
</div>
<div id="ref-godtfredsen2005effect" class="csl-entry" role="listitem">
Godtfredsen, Nina S, Eva Prescott, and Merete Osler. 2005. <span>“Effect of Smoking Reduction on Lung Cancer Risk.”</span> <em>Jama</em> 294 (12): 1505–10.
</div>
<div id="ref-Gupta2021" class="csl-entry" role="listitem">
Gupta, Rishi K, Ewen M Harrison, Antonia Ho, Annemarie B Docherty, Stephen R Knight, Maarten van Smeden, Ibrahim Abubakar, et al. 2021. <span>“Development and Validation of the ISARIC 4C Deterioration Model for Adults Hospitalised with COVID-19: A Prospective Cohort Study.”</span> <em>The Lancet Respiratory Medicine</em> 9 (4): 349–59. <a href="https://doi.org/10.1016/s2213-2600(20)30559-2">https://doi.org/10.1016/s2213-2600(20)30559-2</a>.
</div>
<div id="ref-haber_causal_language" class="csl-entry" role="listitem">
Haber, N. A., S. E. Wieten, J. M. Rohrer, O. A. Arah, P. W. G. Tennant, E. A. Stuart, E. J. Murray, et al. 2022. <span>“<span class="nocase"><span>C</span>ausal and <span>A</span>ssociational <span>L</span>anguage in <span>O</span>bservational <span>H</span>ealth <span>R</span>esearch: <span>A</span> <span>S</span>ystematic <span>E</span>valuation</span>.”</span> <em>Am J Epidemiol</em> 191 (12): 2084–97.
</div>
<div id="ref-harrell2001" class="csl-entry" role="listitem">
Harrell, Frank E. 2001. <em>Multivariable Modeling Strategies</em>. Springer New York. <a href="https://doi.org/10.1007/978-1-4757-3462-1_4">https://doi.org/10.1007/978-1-4757-3462-1_4</a>.
</div>
<div id="ref-Hernan2018" class="csl-entry" role="listitem">
Hernán, Miguel A. 2018. <span>“The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data.”</span> <em>American Journal of Public Health</em> 108 (5): 616–19. <a href="https://doi.org/10.2105/ajph.2018.304337">https://doi.org/10.2105/ajph.2018.304337</a>.
</div>
<div id="ref-hernan2019" class="csl-entry" role="listitem">
Hernán, Miguel A., John Hsu, and Brian Healy. 2019. <span>“A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.”</span> <em>CHANCE</em> 32 (1): 42–49. <a href="https://doi.org/10.1080/09332480.2019.1579578">https://doi.org/10.1080/09332480.2019.1579578</a>.
</div>
<div id="ref-James_Witten_Hastie_Tibshirani_2022" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2022. <em>An Introduction to Statistical Learning: With Applications in r</em>. Springer.
</div>
<div id="ref-kuhn2013a" class="csl-entry" role="listitem">
Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer New York. <a href="https://doi.org/10.1007/978-1-4614-6849-3">https://doi.org/10.1007/978-1-4614-6849-3</a>.
</div>
<div id="ref-Kuhn_Silge_2022" class="csl-entry" role="listitem">
Kuhn, Max, and Julia Silge. 2022. <em>Tidy Modeling with r: A Framework for Modeling in the Tidyverse</em>. O’Reilly.
</div>
<div id="ref-Miller2004" class="csl-entry" role="listitem">
Miller, William C. 2004. <span>“Prevalence of Chlamydial and Gonococcal Infections Among Young Adults in the United States.”</span> <em>JAMA</em> 291 (18): 2229. <a href="https://doi.org/10.1001/jama.291.18.2229">https://doi.org/10.1001/jama.291.18.2229</a>.
</div>
<div id="ref-owidforestsanddeforestation" class="csl-entry" role="listitem">
Ritchie, Hannah, and Max Roser. 2021. <span>“Forests and Deforestation.”</span> <em>Our World in Data</em>.
</div>
<div id="ref-robinsImpossible" class="csl-entry" role="listitem">
Robins, J. M., and L. Wasserman. 1999. <em>On the Impossibility of Inferring Causation from Association Without Background Knowledge</em>. Edited by P. Glymour and G. Cooper. Menlo Park, CA, Cambridge, MA: AAAI Press, The MIT Press.
</div>
<div id="ref-shmueli2010a" class="csl-entry" role="listitem">
Shmueli, Galit. 2010. <span>“To Explain or to Predict?”</span> <em>Statistical Science</em> 25 (3). <a href="https://doi.org/10.1214/10-sts330">https://doi.org/10.1214/10-sts330</a>.
</div>
<div id="ref-steck2021" class="csl-entry" role="listitem">
Steck, Harald, Linas Baltrunas, Ehtsham Elahi, Dawen Liang, Yves Raimond, and Justin Basilico. 2021. <span>“Deep Learning for Recommender Systems: A Netflix Case Study.”</span> <em>AI Magazine</em> 42 (3): 7–18. <a href="https://doi.org/10.1609/aimag.v42i3.18140">https://doi.org/10.1609/aimag.v42i3.18140</a>.
</div>
<div id="ref-Westreich2013" class="csl-entry" role="listitem">
Westreich, D., and S. Greenland. 2013. <span>“The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.”</span> <em>American Journal of Epidemiology</em> 177 (4): 292–98. <a href="https://doi.org/10.1093/aje/kws412">https://doi.org/10.1093/aje/kws412</a>.
</div>
<div id="ref-Wynants2020" class="csl-entry" role="listitem">
Wynants, Laure, Ben Van Calster, Gary S Collins, Richard D Riley, Georg Heinze, Ewoud Schuit, Elena Albu, et al. 2020. <span>“Prediction Models for Diagnosis and Prognosis of Covid-19: Systematic Review and Critical Appraisal.”</span> <em>BMJ</em>, April, m1328. <a href="https://doi.org/10.1136/bmj.m1328">https://doi.org/10.1136/bmj.m1328</a>.
</div>
</div>
</section><aside id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>John Burn-Murdoch was responsible for many of these presentations and gave a <a href="https://cloud.rstudio.com/resources/rstudioglobal-2021/reporting-on-and-visualising-the-pandemic/">fascinating talk on the subject</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>A natural model here is predicting cases, but infectious disease modeling is complex and usually uses techniques outside the usual predictive modeling workflow.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>We say model singular, but usually data scientists fit many models for experimentation, and often the best prediction models are some combination of predictions from several models, called a stacked model<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></aside></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        return container.innerHTML
      } else {
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/chapter-02.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The whole game: mosquito nets and malaria</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>